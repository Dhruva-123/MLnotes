# ğŸ§  Naive Bayes â€” Deep-Dive Notes

---

## I. ğŸ§­ Conceptual Foundations

### ğŸ”¸ What is Naive Bayes?

- A **generative classification algorithm**.
    
- Uses **Bayesâ€™ theorem** + **strong (naive) feature independence** assumptions.
    
- Efficient, scalable, and surprisingly effective on many high-dimensional tasks (like text classification).
    

---

### ğŸ”¸ Bayes' Theorem

$$P(yâˆ£x)=P(y)P(xâˆ£y)P(x)P(y \mid \mathbf{x}) = \frac{P(y) P(\mathbf{x} \mid y)}{P(\mathbf{x})}P(yâˆ£x)=P(x)P(y)P(xâˆ£y)â€‹$$

For classification:

- We ignore the denominator (same for all classes)
    

$P(yâˆ£x)âˆP(y)P(xâˆ£y)P(y \mid \mathbf{x}) \propto P(y) P(\mathbf{x} \mid y)P(yâˆ£x)âˆP(y)P(xâˆ£y)$

We take **log** to avoid numerical underflow:

$$logâ¡P(yâˆ£x)âˆlogâ¡P(y)+logâ¡P(xâˆ£y)\log P(y \mid \mathbf{x}) \propto \log P(y) + \log P(\mathbf{x} \mid y)logP(yâˆ£x)âˆlogP(y)+logP(xâˆ£y)$$

---

### ğŸ”¸ Naive Assumption

Assume features are conditionally independent given the class:

$$P(xâˆ£y)=âˆj=1nP(xjâˆ£y)P(\mathbf{x} \mid y) = \prod_{j=1}^{n} P(x_j \mid y)P(xâˆ£y)=j=1âˆnâ€‹P(xjâ€‹âˆ£y) logâ¡P(xâˆ£y)=âˆ‘j=1nlogâ¡P(xjâˆ£y)\log P(\mathbf{x} \mid y) = \sum_{j=1}^{n} \log P(x_j \mid y)logP(xâˆ£y)=j=1âˆ‘nâ€‹logP(xjâ€‹âˆ£y)$$

This drastically simplifies likelihood estimation.

---

### ğŸ”¸ Probability vs Likelihood

| Term                                         | Meaning | In Naive Bayes                              |
| -------------------------------------------- | ------- | ------------------------------------------- |
| **Probability**                              | P(y     | x) â€” Posterior                              |
| **Likelihood**                               | P(x     | y) â€” Treat data as fixed, class as variable |
| Bayes Rule flips likelihood into probability |         |                                             |

---

## II. ğŸ§© Model Structure (Common to All NB Variants)

python

CopyEdit

`class NaiveBayes:     def fit(self, X, y): ...     def joint_log_likelihood(self, X): ...     def predict(self, X): ...`

- `fit()` estimates prior + likelihood terms.
    
- `joint_log_likelihood(X)` returns $$logâ¡P(y)+logâ¡P(xâˆ£y)\log P(y) + \log P(\mathbf{x} \mid y)logP(y)+logP(xâˆ£y)$$ for each class.
    
- `predict()` selects class with highest log-probability.
    

---

## III. ğŸ“˜ Multinomial Naive Bayes

---

### ğŸ“Œ Use Case

- **Text classification**, word-count features (bag-of-words, n-grams).
    
- Input: non-negative integers (word counts, term frequencies).
    

---

### ğŸ§® Likelihood Estimation

For class y=cy = cy=c, the likelihood of feature xjx_jxjâ€‹ is:

$$Î¸cj=P(xjâˆ£y=c)=Ncj+Î±âˆ‘j(Ncj+Î±)\theta_{cj} = P(x_j \mid y = c) = \frac{N_{cj} + \alpha}{\sum_j (N_{cj} + \alpha)}Î¸cjâ€‹=P(xjâ€‹âˆ£y=c)=âˆ‘jâ€‹(Ncjâ€‹+Î±)Ncjâ€‹+Î±â€‹$$

Where:

- $NcjN_{cj}Ncj$â€‹: Total count of feature jjj in all samples of class ccc
    
- Î±\alphaÎ±: Smoothing constant (Laplace: Î± = 1)
    

---

### ğŸ’¡ Log-Joint Likelihood

$$logâ¡P(y=câˆ£x)âˆlogâ¡P(y=c)+âˆ‘j=1nxjlogâ¡Î¸cj\log P(y = c \mid \mathbf{x}) \propto \log P(y = c) + \sum_{j=1}^n x_j \log \theta_{cj}logP(y=câˆ£x)âˆlogP(y=c)+j=1âˆ‘nâ€‹xjâ€‹logÎ¸cjâ€‹$$

This becomes:

python

CopyEdit

`X @ self.feature_log_prob_ + self.class_log_prior_`

Shape: `(n_samples, n_classes)`

---

### ğŸ§  Intuition

- The log-likelihood is **dot product** of word-counts and log probabilities of words given class.
    
- Multinomial distribution means: count how often each word appears, and treat each occurrence as an independent draw from a vocabulary-specific distribution.
    

---

## IV. ğŸ“— Gaussian Naive Bayes

---

### ğŸ“Œ Use Case

- Continuous numerical features (e.g., sensor data, pixel intensities).
    
- Assumes **features are normally distributed** per class.
    

---

### ğŸ“ Likelihood Model

Each feature xjx_jxjâ€‹ is modeled with a class-specific Gaussian:

$$P(xjâˆ£y=c)=12Ï€Ïƒcj2expâ¡(âˆ’(xjâˆ’Î¼cj)22Ïƒcj2)P(x_j \mid y = c) = \frac{1}{\sqrt{2\pi \sigma_{cj}^2}} \exp\left(-\frac{(x_j - \mu_{cj})^2}{2\sigma_{cj}^2} \right)P(xjâ€‹âˆ£y=c)=2Ï€Ïƒcj2â€‹â€‹1â€‹exp(âˆ’2Ïƒcj2â€‹(xjâ€‹âˆ’Î¼cjâ€‹)2â€‹)$$

---

### ğŸ’¡ Log-Joint Likelihood

After summing over features:

$$logâ¡P(y=câˆ£x)âˆlogâ¡P(y=c)âˆ’12âˆ‘j[logâ¡(2Ï€Ïƒcj2)+(xjâˆ’Î¼cj)2Ïƒcj2]\log P(y = c \mid \mathbf{x}) \propto \log P(y = c) - \frac{1}{2} \sum_j \left[ \log(2\pi \sigma_{cj}^2) + \frac{(x_j - \mu_{cj})^2}{\sigma_{cj}^2} \right]logP(y=câˆ£x)âˆlogP(y=c)âˆ’21â€‹jâˆ‘â€‹[log(2Ï€Ïƒcj2â€‹)+Ïƒcj2â€‹(xjâ€‹âˆ’Î¼cjâ€‹)2â€‹]
$$
---

### âœ³ï¸ Log-Likelihood Vectorized

python

CopyEdit

`log_likelihood = -0.5 * np.sum(     np.log(2. * np.pi * self.var_) + ((X[:, np.newaxis, :] - self.mean_) ** 2) / self.var_,     axis=2 )`

**Shape flow:**

- `X[:, np.newaxis, :]` â†’ shape: (n_samples, 1, n_features)
    
- `mean_` â†’ (n_classes, n_features)
    
- Result â†’ broadcasted to (n_samples, n_classes, n_features)
    
- `sum(axis=2)` â†’ final log-likelihood shape: (n_samples, n_classes)
    

Add priors:

python

CopyEdit

`return log_likelihood + self.class_log_prior_`

---

## V. ğŸ§  General Design Pattern

- All Naive Bayes variants inherit from a `NaiveBayes` base class
    
- Each variant implements `.fit()` and `.joint_log_likelihood(X)`
    
- Base `.predict()` uses:
    
    python
    
    CopyEdit
    
    `table = self.joint_log_likelihood(X) return self.classes_[np.argmax(table, axis=1)]`
    

Optional:

python

CopyEdit

`def predict_proba(self, X):     log_probs = self.joint_log_likelihood(X)     log_probs -= log_probs.max(axis=1, keepdims=True)     probs = np.exp(log_probs)     return probs / probs.sum(axis=1, keepdims=True)`

---

## VI. ğŸ§ª Comparison Summary

|Variant|Feature Type|Model|Use Case|
|---|---|---|---|
|**Multinomial**|Discrete counts|Multinomial|Text classification|
|**Gaussian**|Continuous|Normal distribution|Tabular/numerical sensor data|
|**Bernoulli**|Binary|Bernoulli distribution|Binary word presence (next)|

---

## VII. ğŸ”¥ Youâ€™ve Implemented

- Base class: prediction logic
    
- Full Multinomial Naive Bayes:
    
    - With Laplace smoothing
        
    - Joint log-likelihood vectorized
        
- Full Gaussian Naive Bayes:
    
    - With numerical stability (`+1e-9`)
        
    - Matrix-broadcasted log-likelihood with `axis=2`
        
- Understood dimensions, formulas, log-transform benefits, and independence assumptions