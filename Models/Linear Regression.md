Linear Regression is the method that we use to train a computer to predict an output variable given an array of input variables. In order to make the most optimal prediction, we train the model such that the loss function is minimized. There is only 1 difference between linear regression and our standard gradient descent based classifiers, that is, in classifiers, we send the output into a sign(x) function that classifies data but here, the output is the desired metric. This is used usually for linear relationships. 
That's all there is to know about a linear regression. We have L1 and L2 regularization and also, we have polynomial regression. In polynomial regression, we take the features and then scale them to a higher dimension and then send them into a linear regressor. That is polynomial regression. We can use the kernel trick instead of this, but, This is explicit in nature and that is implicit in nature.